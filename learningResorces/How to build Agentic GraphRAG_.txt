hi everyone uh I hope everyone hears me
well and that everything has started
correctly uh welcome to another MRA
Community call uh I see people are
slowly starting to join so I&#39;ll do a
small intro uh and you&#39;ll get a chance
to uh join others will get the chance to
join as well I see people are saying hi
in the chat so hello to everyone uh
write down where are you joining from
I&#39;m currently in Croatia as well as my
colleague an uh so greetings to all uh
so today uh as I said I have my
colleague an with me an is a developer
experience engineer at mgraph and he
will do presentation on agenty graph Rag
and and he will also showcase the demo
he has been working on uh we were uh
kind of working a lot recently on the
topics of graph rack an AI and maybe you
saw these discussions in the previous
Community calls from us and others who
worked on it uh but yeah the next step I
guess the most next logical step for was
for us to kind of explore agents and
agent to graag more and that&#39;s why we
decided to do this demo and presentation
uh because it was our way of learning
things and now a is going to share his
learnings uh with
us uh so before an starts uh I see you
know where the chat is so please uh do
the chatting there during the whole call
and also if you have a question feel
free to ask there or maybe even better
keep it in the Q&amp;A section where you can
ask a question and then I can highlight
it after the presentation we will do
like a short Q&amp;A session and there will
be a couple of polls also posted in the
poll section so make sure to check that
as well uh I think that&#39;s it I will now
stop talking and uh give the stage to a
uh a welcome and good luck with your
presentation thanks krina um yeah as
Kina did the intro I&#39;ll skip that part
um so um essentially today I will talk
about jent gra um I guess you all um
know what is happening in data world
around gra and general about rag systems
um and now the AI things um just
complicated all that story um so I hope
that today after the presentation you&#39;ll
have a bit more sense of um how I
personally and how we view some things
on topic of the graph frack and
especially with the
agents um so if you have noticed we just
released MRA 3.0 uh we have a vector
search enabled in the new version of MRA
uh we have a availability move to the
production ready we improved the high
availability a lot we scored a lot of
bugs and we&#39;re very proud of it and we
start improving support for dynamic
algorithms so essentially by um
leveraging Dynamic algorithms High
availability and Vector search you can
build a production ready AI
applications and each of these
components um kind of um was the reason
why we decided to uh pump up the maor
version of
M um so um I obviously today use the mro
3.0 and for the rest of the things that
we are going to talk to today um are the
graph the agent the me meent graph and
the the demo my goal is to kind of
clarify this and how you think as I
said so
um graph rag is uh obviously a Hot Topic
these days um but uh it has some
downsides um that I will kind of comment
on um it mainly comes to being rigid and
not being flexible um so this is why we
kind of consider it making it agentic
but I&#39;ll get to that
point for all of you that U didn&#39;t have
a chance to um use the rag systems a lot
I will do just a short intro here so
essentially one of the building blocks
um is the LM um and I guess all of you
know here what DM is it essentially
takes some text promp and generates
something that we want that&#39;s always the
case so we usually want LM to generate
something
useful um and um that means that um um
we typically want to focus on that
prompt we want to have the best we want
to have the best output based on the
prom and it is important to know that
like D LM doesn&#39;t reason in a sense it&#39;s
just text predictor but that&#39;s very
controversial topic we can probably talk
a lot about
that um the goal is like you want to
have some answers answers to the
questions you have so for example like
what patient uh has some symptoms for
example as you can see and you can say
okay I&#39;ll pull all the medical records
and um just give it to LM and decide
like it will decide what the treatment
should be or something like that the
problem is um we have limited Conta
windows and we have a limited span and
that can lead to a little bit poor
results so we need to smartly think
about what are we actually going to
provide into that prompt we can just say
you you you should just take everything
and can make some
decision one of the options is the F
tuning you obviously know the big
companies um that have unlimited budgets
um are doing this um and they&#39;re
essentially um taking the knowledge they
have uh from the internet and from folks
uh and trying to improve those models
now for the companies that are building
uh search um this could maybe not be so
feasible uh because um you need to have
expensive need Machinery to do it you
need to have a bunch of gpus um to train
it and you need to have a bunch of gpus
to actually run it um once that is
once the the network is learned
something it&#39;s pretty fixed but it&#39;s
quite accurate and it can be quite good
for a lot of stuff um especially if you
have a good data but as I said it&#39;s
quite expensive to do it Ro came along
and it provided the new way of doing
things so essentially um it provided
folks to um find the relevant
information provide that into the
context and then from DM we did that
context and um the goal is as I said
having um personalized answers on your
questions and improving the prom to get
the better answer from the llm this is
much easier to set up it&#39;s typically
more faster it&#39;s more flexible uh it is
more scalable and it&#39;s Dynamic of course
the disadvantage is that the um
Precision is probably um um the not so
good good if it train something on a
network by yourself but as I said it&#39;s
resource intensive so a lot of companies
are not able to do
it a short rag intro now the graph rag
is a similar thing but it&#39;s based on the
knowledge graphs um one of the key um
features of that is that you have
Knowledge Graph under the hood that is
feeding the data to the
LM um initially it was developed by the
Microsoft so there was a paper about
Microsoft gra rag and um it was like the
source that spawn a lot of ideas but it
is um critical to mention that this is
not the only approach that you can do
with your graph R um you can do it with
different approaches I will mention some
of them and um we shouldn&#39;t like box
ourself into thinking that it&#39;s based on
loading someone structure data like
documents into the knowledge graph and
then trying to extract knowledge because
typically if you do something like
construction ingestion you&#39;ll get poor
data um not clean data and that can be
PR so initially developed by Microsoft
but um um there are other approaches
that you can leverage obviously M graph
is a graph database um so graph rag as a
concept is very important to M graph and
we want to provide a lot of sport to our
users and customers using graph
frag so when we talk about the MRA graph
rag um the key here is you have a
questions that that question has some
context that needs to be fed and um you
um you assume that you have some kind of
Knowledge Graph already built you have
your private data in your companies and
you want to leverage M graph to perform
a
gra you have to consider like two things
um one could be like P P search and
relevance expansion um and that&#39;s like
finding the exact spot of the data that
is interesting to you and then doing
some expansion based on that point after
you have find that relevant data you you
took it and provide it to the actual LM
and try to answer that question so um
the pivot search and relance expansion
are actually just tools um that you can
leverage so if you think about it the
pivot search could be like a keyword
search a text search a g search a vector
search like a lovian algorithm Liden um
Community detection page rank whatever
graph versals so um it can be anything
you
want and that memb supports from the
tooling of course and um once you
leverage the tools then you can try to
answer that question
um now the example of um Vector search
Pro graph traval is something that um is
one of the tools um for for example you
embed some data it can be embeddings of
the nodes and properties or it can be
embeding of the and then you say okay I
want to perform a vector search on top
of my nodes and find some Sim some
similar nodes and after I find the Sim
the most similar node the nodes that are
the most similar I can do some relevance
expansion and you can use it by you can
kind of do it by using the BFS DFS or
any type of algorithm and the example
would be okay I&#39;m searching for some
patient Mark and then you find it and
then you say okay I want to hop from
Mark to to to some symptoms or I want to
hop one hop away or two hop hops away
and that&#39;s that Vector search rules plus
graph retrieval to get that actual
data actually the um the structure
question that you&#39;re going to ask later
in the demo is essentially Vector search
Pools Plus a graph Ral and this is um um
the Microsoft graph for example they use
um Vector search on top of embeddings of
the documents on communities and then
perform a communities there but it&#39;s a
bit different but as I said the graph
rag is not limited to just one approach
it&#39;s essentially having multi multiple
approaches to to solving the problem
now when we talk about multiple
approaches um um somehow you need to
make decisions on um what tool and what
approach you should use as you can see
MRA has multiple features tools and
somebody has to make a decision if you
hard code like the vector Pro relevance
expansion as a one pipeline um you
basically limit yourself to just that
part of the retrieval
now this is where the agent comes in and
this is where the kind of focus loses a
bit because what is an agent and how it
operates um I like to think about very
simple Concepts and not um going into
theoretical details but this slide is a
bunch of deex that says that agent is a
system component that interacts with
large land image model but if you scroll
if you can look down and read this TDR
it&#39;s an LM call with a specific prompt
and contextual information that makes
some decision it can have multiple steps
so essentially the agent is the exact
same thing as an LM call it shouldn&#39;t be
more complicated than that
usually there are um multiple steps that
can happen there but the important part
is that agent receives some input it&#39;s
takes some decision on that action and
then interacts for example with a no
external nor resource and produces some
response so he is basically making a
decision based on some some data source
similar to the graph R let&#39;s look at an
example let&#39;s say we have a question
classification agent this is the same
part that we&#39;ll have in demo so you ask
a question and then you ask the agent to
classify that question into the
following types because not all the
questions are the same so for example
you can have a retrieval question that&#39;s
some kind of direct lookup where you
look for specific entity or you can have
a like a structure lookup where you kind
of are looking about how the things are
related in the
gra or you can have like a global query
that can follows the trends and looking
like where is the data set going what is
happening with the communities or what
are the most important nodes for example
or you can have like a query about the
database like what how is M configured
at the moment whatever and then um how
old is a person named John okay that&#39;s
withal question that&#39;s the type of the
question that LM made the decision um
and we uh based on that data we are
doing some next steps there and what is
important to know so for example the the
types of question you can ask can be
also Dynamic and it can be pulled from
the database
here&#39;s another another example do MRA
has show schema info enable so
essentially you can Ena show schema info
to get fastpaced um queries about
schema um and um the agent responds in
this case is a databas query now based
on that question or based on that
response you can make some decisions in
your application
an gentic application is a step up from
that it&#39;s essentially um if you again
read just uh tldr it&#39;s a multiple chain
LM score calls where each call is trying
to accomplish some some small task and
they share the context and the knowledge
um in between and if you think about it
a little bit it&#39;s essentially a state
machine um it&#39;s um nothing too fancy uh
but just multiple LM calls
that are sharing the same context and
they&#39;re working together to get to the
actual um solution to the problem the
only difference is that they are um um
making that plan and kind of trying to
autonomously get to some point there&#39;s
no human inter intervention of course
during the process they retrieve the
data and they iterate or have a feedback
loops but uh there are a bunch of uh LM
calls uh chain together to achieve some
common goal and they share how they got
to to specific point to understand how
to um fix
itself um so if you think about it about
the concept of a state machine and in
terms of a
graph you have some
question you could have some context
around that question you provide to that
question to the first agent and agent
makes a decision so he pulls some data
like what type of questions are there
and then he said okay I have made
decision that question is something and
then it passes that to the next agent
who says okay I know how to run a
patrank and I can know how to configure
it and then I will try to run that
patroning and based on the results he
made some decision provides concept
further and then the another agent it&#39;s
also ANM call um does something else and
then you pass that um further so how are
you controlling this um you&#39;re
essentially letting your agents make
decisions you are not controlling the
whole process you are just kind of
creating the guard lines around um what
can happen and what cannot happen um of
course we are not talking here about the
agents iny in Psychology where they have
like free will we are talking more about
they are making a decision we are not uh
forcing the decision on
them and and um um yeah they&#39;re usually
achieved by using the structured output
and functions calling so for all of you
who haven&#39;t had a chance to use the open
AI or API or um link chain and llama
index it&#39;s basically saying n l to
return
uh um Json format or call some specific
function that you have so essentially
alvon return you some kind of random
gber text it will just return that
specific thing you want and in specific
U uh
types and important thing is there are
some feedbacks Loops because um as a
human you also have like that feedback
but I&#39;ll talk a bit about it a bit
later so um why are we talking about
this from the MRA perspective we are
talking because MRA has a bunch of tools
to perform a graph Rag and you want uh
your graph rag to pick the right tool
for the right question that&#39;s
essentially the thing so if I ask you um
or if you have like a question about how
can I get the information does m uses a
log level phrase you will go to
documentation you will search for it Etc
the same way the agent should operate so
so he you ask him you could ask a
question like does MRA has that
configuration type and he should know
what tool to pick to get to that
information
and that&#39;s basically why we are talking
about the Gent graph rag because we want
to gent graph rag to apply also data you
have so applying the right search um and
applying the right tool to the question
you
have and now we are starting to talk
about essentially the demo so not all
our not all gra not all graph tools
responds to all questions as I just said
if you have question about configuration
if you have question about data and that
question about data that can be very
different um you can maybe ask something
about location or
whatever and question essentially
defines the tool that you want to use
but you are not able you are not you
don&#39;t know what tool you will use
because you don&#39;t know what question you
will get so essentially you need to take
let the agent make that
decision um tool is modeled by the agent
automatically and to responses are being
validated and that&#39;s how you can decide
is my answer good or not so essentially
you start with some question the
question classification agent says okay
that&#39;s the question type then tool
selector Regent says okay if that&#39;s the
question type you have this 10 options
or three or two and then you select some
tool and start with that and then
retrieval agents is trying to respond to
that question by perform using that tool
so he&#39;s adapting the tool to the actual
question and then after the tool has
been executed you take some data you
decide is it relevant or not and if if
it is you can say I have the relevant
data I have wred the right context you
should respond to the user and the agent
responds to the user so the only
interaction here as a human is kind of
building this state machine and um
providing the question and everything
else happens uh
automatically here&#39;s the example of that
um classification Ed that di mentioned
um as I said we are using in the demo
structured out outputs from the open AI
you define the class that you want to
get and then he will give you the exact
types you need and then here is the
actual prompt that similar prompt that
we use in the code and he&#39;s making a
choice of what type of question it we
ask and for example if the question was
about most influential nodes
then uh you pick the tools uh you pick
the tool for example page
ring now that agent that manages the
page rank can also say okay I will take
a look at the question and I will decide
how many notes do I need to Pro provide
um if the question is what is the most
important note you will return return
one note if the question is what is the
10 most important notes you he&#39;ll
provide 10 notes if the question is not
specifically defined is the Coca-Cola in
the most influential nodes it will make
a decision on how many nodes he actually
want to provide so here um it&#39;s an LM
call that eventually made some choice
and based on that choice you&#39;re adopting
your tool to that particular
uh uh
outut so when we talk about the demo um
the demo essentially is publicly a
available it should be a a diagnostic as
every demo uh it&#39;s not being built be
battle tested it&#39;s more of a showcase
how you can do it yourself and provide
some
examples and um um after the
presentation you&#39;ll have the link so you
can kind check it out and comment on
it essentially what this demo does is it
operates fully autonomously um it&#39;s
based on the question type and runs a
specific tool that it finds fit for that
question then it retrieves that data
from the MRA via the tool that was
provided and uh Bas on that uh data it
makes the decision and reiterates if it
finds that the the tool has successfully
been
executed and it&#39;s trying to provide best
answer to the question as possible now
we all know that um um it&#39;s hard thing
to cover all the cases so essentially as
a demo I have built just a few types of
questions and answers to Showcase how
this a g State machine operates
autonomously and Def chooses how to pick
the best tool so from the MRA Point know
we essentially use the vector search we
use the Tex to Cipher we use so schema
info we use commun detection page rank
PFS DFS and and a few other things but
essentially the demo does not use
Frameworks so we are not using a l l
chain and llama index why is that the
case because we wanted to iterate fast
and we wanted to get to the greedy needy
details things um what is essentially an
agent care what is um genetic
application like what is how are things
operating because sometimes it is hard
to um see those details if you are using
something that is a few levels highers
in the structions so we essentially go
get and build that by using the open
API so now I&#39;ll show you the demo um
before that um big shout out to Robert
from the S news Who provided us this
data set um although the demo is data
set agnostic and we will continue um
reiterate with different data sets um um
thanks to Robert for providing it for
this particular case so now I will short
shortly shortly jump to the demo um and
start with some questions so for
example um one of the question can be
what can you tell
me about this data
set now if you think about it this is
quite general question and um the
general as it is the general question um
you can use different tools to actually
answer it you don&#39;t need to actually go
and perform something complex
you can actually look at the schema for
example and see what this data set about
what this dat is about and for example
the um classification agent said this is
a database question and then the tool
that can be used is schema tool and it&#39;s
correctly find the right tool to use it
and then it says what this data set
deals about and what are kind of the
statistics of this this data set so
essentially from the point me asking
question to the actual answer I just ask
the question and the tools will selected
automatically this also means that if
something goes wrong it&#39;s not up to me
it&#39;s up to the agents uh trying to
answer this so yeah um yeah the next
question
um uh let&#39;s let go this way what can you
tell
me about
uh so first we are classifying the
question type um as again question is um
quite generic but this type it&#39;s a
retrial type of question the reason why
is retrial because we can specify the
exact entity that you want to foret and
it says that um available tools are
Cipher and Vector relevance expansion
the example that I can show in
slide and the tool was
completely successfully and then we got
the response and Coca-Cola is a company
falls under the category of consumer
goods company so essentially we got the
good response again and this time we use
text to Cipher to get to that particular
uh
solution um okay let&#39;s ask something
more complicated um how is
cocaa connected in the crab
what
is other
companies
whatever so this time it&#39;s a structure
type of question again we have it&#39;s like
a cyppher and vectoral elements tool now
now is
performing one of those tools um and we
will probably get some results um so
essentially as you can see now um
without going into to the details of the
actual results the question defines the
tool the tool is being selected
automatically and we are getting some um
um response so essentially based on the
provided data CCA is connected into
graph to the top gainers to D Jones
Industrial Average so essentially coola
has some connections of gains on
Industrial Average which highlights the
performance some of the BS betweenin the
companies um the interesting thing here
is that we are not um we are not picking
the tool the LM is essentially picking
the tool so um let&#39;s see another one
what level of logging does m has
enabled as you can see it doesn&#39;t have
anything to do with the data set let&#39;s
see if it&#39;s it goes right okay it&#39;s a
database type of question it&#39;s a config
tool that we can use so we got the
config data back this is the m
configuration
from the things you configured while
running the MRA and we got respond MRA
has a loing level set TR as indicated by
configuring
level okay um let&#39;s try to run that um
uh page rank that I was discussing on
the presentation so for example the
question is the Coca-Cola in the most
important nodes like how many nodes what
are the most important nodes we don&#39;t
know so we just as question and LM will
can choose how to interpret this so
essentially this is like Global type of
question where you are considering a
global graph and then um the tool
selected is the page rank or Community
um and P rank was successful and uh we
got few nodes um looks like 10
maybe and uh um based on the provided
data Coca-Cola is not listed among the
most important notes why because LM took
the choice and said okay let&#39;s say that
10 nodes are the most important
depending on how many nodes is actually
here so I didn&#39;t actually configure the
number of nodes here but if I say like U
is the
cocacola in the most thousand important
noes let&#39;s see this this time is is
work so again uh Global question tools
are page rank
community and yes ca-ca is among
thousand most important knows list is a
beverage company with a rank of
something right so um we essentially got
thousand notes back uh because the llm
adapted the query graph R query to the
actual content of the question we didn&#39;t
um um pass the parameter but rather we
askm to decide how many noes is actually
considered the most important or is is
there anything that suggested that what
is the the category limit for that for
example okay that&#39;s pretty much it if
you are going to have any questions we
can also ask the um
graag like let&#39;s say something random to
see if it&#39;s going to fail what is
connection
between Chinese
stock
market and us I don&#39;t know what will
happen so it&#39;s a structure type of
question um which is good we are asking
about the structure the possible tools
are Cipher or vector Elance expansion in
this
case and we&#39;ll see what we are going to
get if you&#39;re going to get
anything but the Point here is um this
you see the cyh has failed so the the
the relevance expansion was successful
we actually CED the the data back now
it&#39;s generating the final
response okay um so obviously um the
Federal Reserve is an important entity
and um yeah we got some um results I&#39;m
not sure how truth this result is but
it&#39;s um mentioning interest rates
adjustments um having impact in both us
and um Chinese STS so it looks like
something that is a solid answer but
what is important here to note we as
some general question the structure tool
was used the one to one tool has failed
Cipher was not able to generate that but
then we uh go back to the second tool
and the tool actually succeeded and
based on there the data that we got back
we essentially got some answer to our
graph R
qu yeah so what is important here um
important is that you need to have that
feedback loops uh to be able to handle
that cases when the things go wrong and
this is an interesting example that I
screenshotted so that we don&#39;t need to
repeat it here is um the question is
what can you tell me about the Coca-Cola
ENT essentially the same thing happened
in the background now
is it&#39;s a retrieval type of question and
U Cypher is trying to um match that uh
Coca-Cola entity so if you take a look
sorry if you take a look entity
organization ID Coca-Cola and is trying
to match that um um entity based on
schema but the problem is the query
didn&#39;t not return any results the query
actually was okay it was a member was
able to execute the query but there
wasn&#39;t any data so the problem here is
in the uppercase or lowercase letters so
when the human type and ask questions
they don&#39;t care about case sensitivity
so it is important to build in um some
type of retry me mechanism for L to be
able to kind of try to figure out what
is the problem so in the actual code you
can kind of say like I said here there&#39;s
a possible issue with the qu labels and
parameters if you&#39;re matching strings
consider matching them in the case in
sensitive a so the agent is kind of with
the promps trying to figure out the way
to get um better and U recover itself
and try to answer that and if you take a
look at this particular example um it
got right so um based on the correct
quity it was able to get that Coca-Cola
and theb back but in general the
feedback loops are important and they
can be implemented for any tools that
you have um pank for example
also so um we build this to to learn to
to share that knowledge and we&#39;ll
continue to build share knowledge with
you um and when we kind of finished we
were thinking about possible
improvements um so it be cool to have
like Geo tools text Dynamic C gos all
built um and that
all AO with that we have fullly
automatic tool selection so now I have
limited tool selection based on type of
the question but having like a full list
of tools and then just asking the
question and then just finding the right
tool that is being uh specifically
managed by specific agent for that
particular tool would be super cool and
essentially you will build a universal
MRA knowledge matal agent that can
retrieve Knowledge from any graph for
example one of the things that we
already have is the graph chat for
example and the graph chat is
essentially text to Cipher agent and it
was built to support your queries in
English and then it&#39;s trying to um um
generate the cipher equivalent of that
English question and it&#39;s do quite fine
but uh if you ask it like what are the
most important notes it may kind of
generate the um
the page rank or it may not you are not
sure but um the point is is is just one
of the tools that you can use to build
um gentic graph it&#39;s not just about the
text to Cipher or it&#39;s just one of the
possible tools so everything we learn
will probably build buil into graph chat
to be more generic uh so not to use just
text
Cy and yeah uh so this pretty much SS of
what I wanted to say before we move to
Q&amp;A I just wanted to say that you have a
chance to listen to member of agent gra
today um February uh 18 we have a gra
gra Global sear modeling then after that
we have intro to mangra FL 2.0 and demo
which the graph chat will be one of the
topics and how we improved it um and
actually the B CH is getting very good
is getting to be a very good agent for
Cy generation and then you&#39;ll talk about
MRA
nip and retrial augment duration from fi
Consulting and I did mention the is from
Redfield on the global search
with um yeah now I will move
to uh Q&amp;A if there are any questions
thank you an uh just a sec let me fix
this us a bit
okay um yeah we have questions we had a
lot of questions and a lot of
conversation in the chat but let me
start highlighting uh some of those uh
so I shared the demo code there were
questions about that but maybe to add to
that question um so s ask if you are not
using Lang chain or llama index how do
you know which tool is being called I
know you commented on this one maybe one
more time to explain this to
everyone so one of the reasons why we
didn&#39;t use linkchain and Lama index
they&#39;re great Frameworks but um they
essentially create that these
abstractions on top of llm calls and
then you&#39;re not sure like what logic is
being implemented in Lang chain and
llama index and you&#39;re not sure like how
all these things operate and work
together by using directly the API in
this concrete example we use the open AI
API uh we were able to control the whole
process and we were able to use the
structured outputs um uh I experimented
with the function calling but I didn&#39;t
use it because it&#39;s much easier to
prototype with structured
outputs um to build it from the um
ground and um yeah it&#39;s maybe not so
reusable for the folks who use llama
index and length chain uh because it
doesn&#39;t have direct examples but it&#39;s
provided a good knowledge base so
yeah if you if you&#39;re not using link
chain or llama index the open AI or I
think pretty much any a API from these
um LM Cloud providers can give you the
structured output for example and you
get Json back and you can do it whatever
want so you basically use that for this
tool selection yeah okay um okay uh then
uh let&#39;s go next uh we have a question
when you mentioned that issues on graph
rack are that they are a bit more static
compared to the other rag Solutions uh
then Felipe ask did we explore or have
suggestions on ways of making it easy to
over overcome that so like what kind of
solutions patterns would be to
regenerate graph structures what is our
approach for that because from his
conversation with friends uh blocker for
adoption was what if my graph is wrong
and I want to do big changes on it so
what if the graph changes how how the do
we handle that in MRA and what would we
kind of suggest in that case yeah I&#39;m
not I&#39;m not sure I&#39;m not super sure that
I understand the correct the the
question correctly but I will try to
provide the answer so uh when I said St
static I um meant the pipeline your in
your graph rag is probably static so
essentially you decided to Sol to answer
all the question with a particular
pipeline where you say okay I will use
embeddings I will find the relevant part
of the graph and I will perform um
relevance expansion from that and U but
what if you what if you uh use um some
synonym for example for the same thing
and then the embedding kind failed and
maybe the text search would catch
something similar so you&#39;re not sure
what tools will answer you to your
question and that&#39;s why what I meant by
the
static um when we talk about the actual
graphs structures if Philip on that um
the problem with the graph structures is
yeah they&#39;re Dynamic and it&#39;s hard to uh
track the schema um and adapt the
queries and pipelines to the changing
graph schematics but um the this is
essentially where the texto cipher
really shines because no matter no
matter the schema it essentially
recognizes the changes and adapts the
queries directly that&#39;s one of the
benefits but this is also a point where
the cipher is being gener
corrected uh thank you an and when
talking uh about Cypher we have a couple
of questions regarding that so s asked
but I saw that in one of the following
slides is it possible to see the cyer
quy that is generated so during this
process do you have that control of when
uh agent select the tool and you have
some Cipher that is generated in the
background can you kind of see what&#39;s
actually being run in the
background yeah so you are you are
providing um um you&#39;re providing a a
prompt to an nlm you can call it an just
an llm call or you can call it an agent
whatever kind of suits you um you&#39;re
providing a PR to the llm and you are
asking to generate the cipher and then
you get that structured output in form
of a query so you can immedi see what
query did you get but uh at that point
um you can do a validation of that that
query by trying to run it on M graph
then if you got some results you are
considering is it good results um that&#39;s
the feedback look that I was mentioning
or thinking about if I didn&#39;t get any
results um but I&#39;m kind of confident
there should be some results maybe
something is being wrongly generated and
then you need to think like like how the
person would essentially write the wrong
quy and uh how you can help an llm to
figure it out and it will work in the
some cases of course Cypher generation
won&#39;t work in uh in all the cases and
and one of the things in MRA that we&#39;ll
work on in the future months is proving
the error messages from the wrong Cipher
queries because they provide valuable
context back to those LM or agents
trying to write that an example here is
the rust programming language for
example they have a great error messages
um for a user um that uses the
programming language um it would be cool
if Cipher
typings uh thank you an and regarding
Cipher so can you maybe explain the
difference because I saw the question
here uh what is meant for instance the
cipher tool that was used in the demo so
can you maybe give us an explanation of
when you talk to about text to Cipher or
like a cipher tool versus a cipher query
that is generated by other tools uh that
you have in the uh in your pipeline yeah
yeah so um essentially when we said
Cipher Tool uh That&#39;s essential the text
to Cipher generation and uh that&#39;s
essentially what the lab Lab does is I
said it&#39;s just one tool that you can
leverage to try to answer the question
and yeah if you can if as we showed in
the demo we asked some question and The
Cypher generation was unsuccessful even
after like four or I I think that
default is for each RSE um it didn&#39;t
manage to recover itself um even though
MRA probably provided the exception like
we don&#39;t support that function or I
didn&#39;t look now in the dets of the logs
but then it uh decided to use another
Tool uh because this tool wasn&#39;t
unsuccessful so U yeah um that&#39;s that
was referring to the same
thing okay I hope that answers this
question uh so uh we have another
question uh are graphs created using
llms if so what is the cheapest way to
create an accurate
graphs if graphs are created using L um
so
um it&#39;s definitely it&#39;s definitely some
kind of um not online model um from the
apis because typically they&#39;re quite
expensive um and yeah so if you can host
your own smaller models um they can do
quite good um knowledge extractions I
think like Spacey um and uh has some
like libraries to to help with that so
um um essentially use not using the
uh cloud provider API to do it would be
much much cheaper if you can run the L
locally or something like that yeah and
as Robert just commented so the graph
that was used in today&#39;s demo uh is the
ask news data set he Pro provided and he
just shared the model he used in the
chats so by PR many so yeah of course
you can use a model to do that um the
point of this demo was more the focus on
the different retrieval strategies and
creating agents and agenty graph rag but
definitely Knowledge Graph creation and
we had some questions about data
modeling it&#39;s a topic for itself I think
this is saying that we should do that
one as well uh a bit of uh additional
research on that side and then do
another call on that topic as it&#39;s quite
I would say quite Hot
Topic uh we have one more question from
Ali uh so uh he said he still doesn&#39;t
see what&#39;s the use case here so what are
the actual benefits of even having a
graph rag or like maybe you can compare
having gent graph rag
yeah so um if you think about search in
your company um um I&#39;m not sure where
Ali works but if you work in a some
company midsize company that has a lot
of internal data and um you want to
figure out some things in data or ask
about data um and uh you are not sure
what is the best approach to get to that
data now um you don&#39;t need to think
about how to get that data if you have
autonomous gra system that decid that
for you and is trying to do the best
thing it can to find that data that you
need so essentially the engineers can be
can build a graph pipeline that is fixed
uh and that fixed pipeline will work in
some questions if you have like a
autonomous graph pipeline that&#39;s being
driven by a gentic LMS for example it
could maybe work in case of in cases
that have like multiple question types
and be more adaptive to that question
so when we talk about the exact exact
benefits um we cannot pinpoint and say
like this is the benefit one um because
um it&#39;s very early days for graph Rack
in general and also for agent graph rack
so
yeah yeah thank you I think that&#39;s a
quite good answer I hope that helps uh
ali uh and we had another one that&#39;s
maybe I mean it&#39;s more related to the
thing that I was mentioning sooner
previously I think what about a the
medic entity identification for
technical documents maybe this is more
again referring I&#39;m not sure I would say
it&#39;s maybe more referring to the uh
named entity recognition process and
maybe building a graph um any comments
on that yeah I think that
um for example the one of the shout outs
that we did uh for the ask news for
Robert so he&#39;s essentially building a
lot of this um Dynamic data said from
the news and I think this is a very hard
task especially entity resolution um
where you are trying to match the the
data that you already have in the
database to some entity that is being
driven from the LM um and in general I
think that as we are generating un
structured data with llms we&#39;ll get semi
good data into the database uh which is
somewhat expected but uh uh we should
also treat it that way and we should be
like um suspicious of is that something
that&#39;s relevant or um how like we should
be suspicious is the data correct now um
what is avilable here is that
essentially and for example in that as
news case is if you have really fresh
data you can kind of quickly scrape it
and load it and you can maybe make some
fast decisions um based on the newest
events so there is a benefit of not
having uh everything structured and uh
not having everything nicely polished
because they this takes time but if you
have something that you need to do quite
fast in the timely manner you can just
um ingest
everything how it is in whatever format
it is and then try to make some
decision yeah definitely a big topic to
talk about yeah that&#39;s that&#39;s a limited
topic to talk about
unlimited uh okay uh so uh we have
another question how does it perform I
guess your agentic graph rag when you
ask analytics questions such as how many
companies there are so questions about
averages also or fuzzy questions like
what happened last
week
uh okay so this a good question um so um
this graph rag uh graph rag that we just
showcased is limited to a set of tools
that
provided and essentially um if I ask how
many companies there are um there will
be probably two approaches there um for
example it could generate a cipher and
try to match and count the companies it
could go to the schema info because
schema has the number of nodes that has
a company label on it for example so
that&#39;s how it will
uh answer about uh companies now how
would it that&#39;s about last week that&#39;s a
good question um because it&#39;s a general
concept of time uh in the data set and
I&#39;m not sure like what tool would be
used here but it will be interesting to
see like we have this um temporal
features like daytime Etc so maybe um uh
some Community uh on top of based on
some data from the day times could be
leveraged there but uh that&#39;s the beauty
of of having autonomous things is you
are not in control of what will be
called so for that part for this
particular demo I cannot answer but for
like generally if you talk generally
providing the tools to the gra system
and let it make a decision of
that yeah yeah definitely and uh as
Robert said like in the comments he said
I think if time is important we could
add it to the onology so yeah again it
circles back to how you model data
because you need to ask this kind of
question I mean it&#39;s not easy to cover
all kinds of questions that anyone like
that pops your mind like basically if
it&#39;s a business use case you probably
have a set of questions that you kind of
expect your users to ask and yeah I
don&#39;t know definitely not easy one yeah
this is a good point from Robert so for
example having a geolocation data also
and then you maybe you can model the
whole graph based on how things are
distance apart from each other and then
everything is built into
but uh yeah interesting yeah yeah
definitely uh I would say thank you an I
think we can wrap up there were a lot of
questions a lot of good questions thank
you everyone for asking them for joining
if you will have more join our Discord
uh we have a Discord Community uh that
quite vibrant you can ask questions
there a is also there every day so no
worries you can ask him anything you
like I shared the presentation here and
the code here um but uh I will also so
uh we will also probably send this as a
followup um and okay let let&#39;s do one
more I see there is one more so we don&#39;t
miss it how deterministic are the
answers that the agentic graph rag will
give you are they quite consistent given
certain degree of variability in the
input promise okay this is the last one
I promise yeah yeah so they&#39;re not
deterministic uh because if you change
just a one word in your question it can
be difference for example it can be a
difference between
what can you like how many important
nodes are there or what are the 10 most
important nodes so essentially you
immediately change the the the the the
problem there and then the LM will react
completely differently um and not just
that so as the LM is not deterministic
you cannot say that even asking the same
question will be deterministic uh so
yeah um if you have similar questions
you will get similar let&#39;s say DET semi
deterministic answers and tools used but
yeah if you change anything it can Wily
be
different yeah it&#39;s not easy to be
always sure what what will actually
happen in the end but having a lot of
these tools and feedbacks feedback loops
and stuff you can kind of have more
control over that
process uh so uh as an said we have a
lot of events coming up uh during
February uh we have three more now
February please check our our website
you have all the all the information
there register for the next Community
call with Redfield uh and I guess I&#39;ll
see you there very soon thank you an
once again for doing this presentation
it was awesome thank you for answering
all of the questions to our community uh
and that&#39;s it thank you everyone for
joining and see you really soon on the
next member of community call thank you
bye bye